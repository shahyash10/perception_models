# We use a global batch size of 2048 in stage # 3 for PLM-8B model. Please adjust batch_size as per your training setup.
# For example, one possible configuration is batch_size=4,nodes=64,gpus_per_node=8 = 4*64*8 = 2048 global batch size.

name: "plm_8b_stage2"
dump_dir: /fsx-checkpoints/yashs/plm/plm_8b_cambrian_stage2_baseline
steps: 7000 # (7000000)/(8*16*8) ~ 7,000
seed: 777
optim:
    lr: 2e-5 # global bz -> 2048/2, lr /= 2
    warmup: 120
    lr_min_ratio: 0.01
    clip: 1.0
    weight_decay: 0.05

distributed:
    fsdp_type: full_shard
    compile: false
    model_dtype: bf16
    matmul_allow_tf32: false
    selective_activation_checkpointing: false
    full_activation_checkpointing: true
    tp_size: 1

model:
    dim: 4096
    n_layers: 32
    n_heads: 32
    n_kv_heads: 8
    vocab_size: 128256
    ffn_dim_multiplier: 1.3
    multiple_of: 1024
    norm_eps: 1e-05
    rope_theta: 500000.0
    weight_tying: false
    max_seqlen: 6144
    freeze_language_model: false
    freeze_vision_model: false
    pooling_ratio: 2
    vision_model:
        image_size: 448
        patch_size: 14
        width: 1536
        layers: 47
        heads: 16
        use_cls_token: false
        use_abs_posemb: true
        mlp_ratio: 5.833333334
        ls_init_value: 0.1
        drop_path: 0.1
        use_ln_post: false
        pool_type: "none"
    mlp_init:
        use_gaussian: true

data:
    datamix: Cambrian7M-Shuffled:1.0
    num_workers: 4
    batch_size: 8
    image_res: 448
    max_num_tiles: 16
    max_video_frames: 16
    vision_input_type: thumb+tile
    tokenizer_path: facebook/Perception-LM-8B/tokenizer.model
    tokenizer_name: plmchat
    conversation_format: plm_sft

profiling:
    run: false

checkpoint:
    dump:
        every: 500
        keep: 1
    init_ckpt_path: /fsx-checkpoints/yashs/plm/plm_8b_cambrian_stage1_shuffled/checkpoints/0000008000/
    is_consolidated_model: True

logging:
    freq: 10
    level: INFO  # Available choices for logging level are: [NOTSET, DEBUG, INFO, WARN, ERROR, FATAL, CRITICAL]
    wandb:
        project: stage2
        entity: plm
        group: finetune
        name: plm_8b_cambrian_stage2_baseline
        tags:
        - plm_8B
        - stage2